{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae79765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score,log_loss,precision_score,recall_score,make_scorer,accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(data['sentiment'])\n",
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the TfidfVectorizer to transform the textual data in the 'filtered_review' column of your dataset \n",
    "# into a matrix of numerical features.The resulting matrix 'x' has shape (n_samples, n_features) and is used as input to train the classifiers.\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(data['review'])\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bee752",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score),\n",
    "           'recall': make_scorer(recall_score),\n",
    "           'f1': make_scorer(f1_score),\n",
    "           'log_loss': make_scorer(log_loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee72ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines and evaluates the performance of a Logistic Regression classifier on the provided dataset \n",
    "# using cross-validation. The average scores for various evaluation metrics on both training and test sets are printed.\n",
    "\n",
    "\n",
    "lr = LogisticRegression(random_state = 0, max_iter = 5)\n",
    "lr_scores = cross_validate(lr, x, y, cv=5,scoring=scoring,return_train_score=True, verbose = 3, n_jobs = -1)\n",
    "print(\"The Fit Time is: \",lr_scores['fit_time'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Score Time is: \",lr_scores['score_time'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Accuracy score is: \",lr_scores['train_accuracy'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train F1 score is: \",lr_scores['train_f1'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Precision score is: \",lr_scores['train_precision'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Recall score is: \",lr_scores['train_recall'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Log Loss is: \",lr_scores['train_log_loss'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Accuracy score is: \",lr_scores['test_accuracy'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test F1 score is: \",lr_scores['test_f1'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Precision score is: \",lr_scores['test_precision'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Recall score is: \",lr_scores['test_recall'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Log Loss is: \",lr_scores['test_log_loss'].mean())\n",
    "print(\"\\n\")\n",
    "lr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2662606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fits a decision tree classifier with maximum depth of 25 and performs cross-validation with 5 \n",
    "# folds, computing various performance \n",
    "# metrics on both the training and test sets. The results are printed and stored in a dictionary called \"dt_scores\".\n",
    "\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = 0, max_depth = 25)\n",
    "dt_scores = cross_validate(dt, x, y, cv=5,scoring=scoring,return_train_score=True, verbose = 3, n_jobs = -1)\n",
    "print(\"The Fit Time is: \",dt_scores['fit_time'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Score Time is: \",dt_scores['score_time'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Accuracy score is: \",dt_scores['train_accuracy'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train F1 score is: \",dt_scores['train_f1'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Precision score is: \",dt_scores['train_precision'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Recall score is: \",dt_scores['train_recall'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Log Loss is: \",dt_scores['train_log_loss'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Accuracy score is: \",dt_scores['test_accuracy'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test F1 score is: \",dt_scores['test_f1'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Precision score is: \",dt_scores['test_precision'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Recall score is: \",dt_scores['test_recall'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Log Loss is: \",dt_scores['test_log_loss'].mean())\n",
    "print(\"\\n\")\n",
    "dt_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61cbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines a random forest classifier and performs cross-validation on the given dataset, \n",
    "# calculating various metrics for both training and\n",
    "# testing sets. The mean values of these metrics are then printed along with the fit and score times.\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 0, max_depth = 25)\n",
    "rf_scores = cross_validate(rf, x, y, cv=5,scoring=scoring,return_train_score=True, verbose = 3, n_jobs = -1)\n",
    "print(\"The Fit Time is: \",rf_scores['fit_time'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Score Time is: \",rf_scores['score_time'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Accuracy score is: \",rf_scores['train_accuracy'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train F1 score is: \",rf_scores['train_f1'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Precision score is: \",rf_scores['train_precision'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Recall score is: \",rf_scores['train_recall'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Log Loss is: \",rf_scores['train_log_loss'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Accuracy score is: \",rf_scores['test_accuracy'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test F1 score is: \",rf_scores['test_f1'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Precision score is: \",rf_scores['test_precision'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Recall score is: \",rf_scores['test_recall'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Log Loss is: \",rf_scores['test_log_loss'].mean())\n",
    "print(\"\\n\")\n",
    "rf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates an instance of the MLPClassifier with certain hyperparameters, then uses cross-validation to evaluate \n",
    "# its performance on the given dataset, and prints out the mean scores for each metric. The results indicate the average \n",
    "# performance of the MLPClassifier model on the dataset, based on various evaluation metrics such as accuracy, F1 score, and precision.\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(150, 15, ), random_state=0, max_iter = 5)\n",
    "mlp_scores = cross_validate(mlp, x, y, cv=5,scoring=scoring,return_train_score=True, verbose = 3, n_jobs = -1)\n",
    "print(\"The Fit Time is: \",mlp_scores['fit_time'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Score Time is: \",mlp_scores['score_time'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Accuracy score is: \",mlp_scores['train_accuracy'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train F1 score is: \",mlp_scores['train_f1'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Precision score is: \",mlp_scores['train_precision'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Recall score is: \",mlp_scores['train_recall'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Train Log Loss is: \",mlp_scores['train_log_loss'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Accuracy score is: \",mlp_scores['test_accuracy'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test F1 score is: \",mlp_scores['test_f1'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Precision score is: \",mlp_scores['test_precision'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Recall score is: \",mlp_scores['test_recall'].mean())\n",
    "print(\"\\n\")\n",
    "print(\"The Test Log Loss is: \",mlp_scores['test_log_loss'].mean())\n",
    "print(\"\\n\")\n",
    "mlp_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Neural Network']\n",
    "train_acc = [lr_scores['train_accuracy'].mean(), dt_scores['train_accuracy'].mean(), rf_scores['train_accuracy'].mean(), mlp_scores['train_accuracy'].mean()]\n",
    "test_acc = [lr_scores['test_accuracy'].mean(), dt_scores['test_accuracy'].mean(), rf_scores['test_accuracy'].mean(), mlp_scores['test_accuracy'].mean()]\n",
    "train_prec = [lr_scores['train_precision'].mean(), dt_scores['train_precision'].mean(), rf_scores['train_precision'].mean(), mlp_scores['train_precision'].mean()]\n",
    "test_prec = [lr_scores['test_precision'].mean(), dt_scores['test_precision'].mean(), rf_scores['test_precision'].mean(), mlp_scores['test_precision'].mean()]\n",
    "train_recall = [lr_scores['train_recall'].mean(), dt_scores['train_recall'].mean(), rf_scores['train_recall'].mean(), mlp_scores['train_recall'].mean()]\n",
    "test_recall = [lr_scores['test_recall'].mean(), dt_scores['test_recall'].mean(), rf_scores['test_recall'].mean(), mlp_scores['test_recall'].mean()]\n",
    "train_f1 = [lr_scores['train_f1'].mean(), dt_scores['train_f1'].mean(), rf_scores['train_f1'].mean(), mlp_scores['train_f1'].mean()]\n",
    "test_f1 = [lr_scores['test_f1'].mean(), dt_scores['test_f1'].mean(), rf_scores['test_f1'].mean(), mlp_scores['test_f1'].mean()]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "axs.plot(models, train_acc, label='Train Acc')\n",
    "axs.plot(models, test_acc, label='Test Acc')\n",
    "axs.plot(models, train_prec, label='Train Prec')\n",
    "axs.plot(models, test_prec, label='Test Prec')\n",
    "axs.plot(models, train_recall, label='Train Recall')\n",
    "axs.plot(models, test_recall, label='Test Recall')\n",
    "axs.plot(models, train_f1, label='Train F1')\n",
    "axs.plot(models, test_f1, label='Test F1')\n",
    "\n",
    "axs.set_xlabel('Model')\n",
    "axs.set_ylabel('Score')\n",
    "axs.legend()\n",
    "axs.set_title('Performance Metrics for Four Models for TF-IDF Features')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [lr_scores['train_log_loss'].mean(), dt_scores['train_log_loss'].mean(), rf_scores['train_log_loss'].mean(), mlp_scores['train_log_loss'].mean()]\n",
    "test_loss = [lr_scores['test_log_loss'].mean(), dt_scores['test_log_loss'].mean(), rf_scores['test_log_loss'].mean(), mlp_scores['test_log_loss'].mean()]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "axs.plot(models, train_loss, label='Train Loss')\n",
    "axs.plot(models, test_loss, label='Test Loss')\n",
    "\n",
    "axs.set_xlabel('Model')\n",
    "axs.set_ylabel('Log Loss')\n",
    "axs.legend()\n",
    "axs.set_title('Train and Test Log Loss for Four Models for TF-IDF Features')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb785a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
